{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# (1) Streaming Meetups to IBM Cloudant using Spark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook we are going to utilize the [IBM Apache Spark](https://console.ng.bluemix.net/catalog/services/apache-spark/) and [IBM Cloudant](https://console.ng.bluemix.net/catalog/services/cloudant-nosql-db/) [Bluemix](https://console.ng.bluemix.net/) services to process and persist data from the [Meetup rsvp stream](http://www.meetup.com/meetup_api/docs/stream/2/rsvps/).  On the backend the IBM Apache Spark service will be using the the [Spark Kernel](https://github.com/ibm-et/spark-kernel).  The Spark Kernel provides an interface that allows clients to interact with a Spark Cluster. Clients can send libraries and snippets of code that are interpreted and run against a Spark context.  This notebook should be run on IBM Bluemix using the IBM Apache Spark and IBM Cloudant services available in the [IBM Bluemix Catalog](https://console.ng.bluemix.net/catalog/)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prerequisites\n",
    "* IBM Bluemix account - [Sign up for IBM Bluemix](https://console.ng.bluemix.net/registration/)\n",
    "* IBM Spark service instance - [Create an IBM Apache Spark Service](https://console.ng.bluemix.net/catalog/apache-spark/). Note once the IBM Spark service is created you can then create a Notebook on top of the IBM Spark service.\n",
    "* IBM Cloudant service instance - [Create a IBM Cloudant NoSQL DB Service](https://console.ng.bluemix.net/catalog/cloudant-nosql-db/).  Note once the IBM Cloudant service is created you can then create a meetup_group database by launching the IBM Cloudant service, navigating to the databases tab within the dashboard, and selecting create database.\n",
    "* IBM Cloudant service credentials - Once in a notebook use the Data Source option on the right Palete to Add a data source.  After the data source is configured and linked to the created notebook you can use the Insert to code link which will add metadata regarding the datasource to your notebook.  You will want to keep track of the hostname, username, and password to be used for configuration in step (2) below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## 1. Add Dependencies/Jars\n",
    "In order to run this demonstration notebook we are using the cloudant-spark and scalawebsocket libraries. These scala dependencies/jars are added to our environment using the AddJar magic from the Spark Kernel, which adds the specified jars to the Spark Kernel and Spark cluster.\n",
    "* scalawebsocket - Used for streaming data\n",
    "* async-http-client - Used for streaming data\n",
    "* cloudant-spark - Allows us to perform Spark SQL queries against RDDs backed by IBM Cloudant\n",
    "* scalalogging-log4j - Logging mechanism"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cached version of scalawebsocket_2.10-0.1.1.jar\n",
      "Using cached version of cloudant-spark.jar\n",
      "Using cached version of async-http-client-1.8.0.jar\n",
      "Using cached version of scalalogging-log4j_2.10-1.1.0.jar\n"
     ]
    }
   ],
   "source": [
    "%AddJar http://central.maven.org/maven2/eu/piotrbuda/scalawebsocket_2.10/0.1.1/scalawebsocket_2.10-0.1.1.jar\n",
    "%AddJar https://dl.dropboxusercontent.com/u/19043899/cloudant-spark.jar\n",
    "%AddJar http://central.maven.org/maven2/com/ning/async-http-client/1.8.0/async-http-client-1.8.0.jar\n",
    "%AddJar http://central.maven.org/maven2/com/typesafe/scalalogging-log4j_2.10/1.1.0/scalalogging-log4j_2.10-1.1.0.jar"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Initialize Spark context with IBM Cloudant configurations\n",
    "The IBM Bluemix Apache Spark service notebook comes with a Spark context ready to use, but we are going to have to modify this one to configure built in support for IBM Cloudant.  Note for the demo purposes we are setting the Spark master to run in local mode, but by default the IBM Spark service will run in cluster mode.  Update the HOST, USERNAME, and PASSWORD below with the credentials to connect to your IBM Bluemix Cloudant service which our demo depends on.  You can get these credentials from the Palette on the right by clicking on the Data Source option.  If your data source does not exist add it using the Add Source button or if it already does you can use the \"Insert to code\" button to add the information to the notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import org.apache.spark.rdd.RDD\n",
    "import org.apache.spark.sql.{DataFrame, SQLContext}\n",
    "import org.apache.spark.storage.StorageLevel\n",
    "import org.apache.spark.streaming.{Time, Seconds, StreamingContext}\n",
    "import org.apache.spark.{SparkConf, SparkContext}\n",
    "\n",
    "val conf = sc.getConf\n",
    "conf.set(\"cloudant.host\",\"HOST\")\n",
    "conf.set(\"cloudant.username\", \"USERNAME\")\n",
    "conf.set(\"cloudant.password\",\"PASSWORD\")\n",
    "\n",
    "conf.setJars(ClassLoader.getSystemClassLoader.asInstanceOf[java.net.URLClassLoader].getURLs.map(_.toString).toSet.toSeq ++ kernel.interpreter.classLoader.asInstanceOf[java.net.URLClassLoader].getURLs.map(_.toString).toSeq)\n",
    "conf.set(\"spark.driver.allowMultipleContexts\", \"true\")\n",
    "conf.set(\"spark.master\",\"local[*]\")\n",
    "val scCloudant = new SparkContext(conf)\n",
    "scCloudant.getConf.getAll.foreach(println)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Write to the IBM Cloudant Bluemix service\n",
    "Using the cloudant-spark library we are able to seemlessly interact with our IBM Cloudant Bluemix Service meetup_group database through the abstraction of Spark Dataframes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def writeToDatabse(databaseName: String, df: DataFrame) = {\n",
    "    df.write.format(\"com.cloudant.spark\").save(databaseName)\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Create the WebSocketReceiver for our Streaming Context\n",
    "First we must create a custom WebSocketReceiver that extends [Receiver](http://spark.apache.org/docs/latest/streaming-custom-receivers.html \"Spark Streaming Custom Receivers\") and implements the required onStart and onStop functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import org.apache.spark.storage.StorageLevel\n",
    "import scalawebsocket.WebSocket\n",
    "import org.apache.spark.streaming.receiver.Receiver\n",
    "import org.apache.spark.Logging\n",
    "\n",
    "import org.json4s._\n",
    "import org.json4s.DefaultFormats\n",
    "import org.json4s.jackson.JsonMethods._\n",
    "\n",
    "case class MeetupEvent(event_id: String,event_name: Option[String],event_url: Option[String],time: Long)\n",
    "case class MeetupGroupTopics(topic_name: Option[String],urlkey: Option[String])\n",
    "case class MeetupGroup( group_id: Long, group_name: String,group_city: Option[String],group_country: Option[String], group_state: Option[String],  group_urlname: Option[String], group_lat: Option[String],group_lon: Option[String],group_topics: List[MeetupGroupTopics]) \n",
    "case class MeetupMember( member_id: Long, member_name: Option[String],other_services: Option[String],photo: Option[String])\n",
    "case class MeetupVenue(venue_id: Long, venue_name: Option[String],lat: Option[String], lon: Option[String])\n",
    "case class MeetupRsvp(rsvp_id: Long,response: String,guests: Int, mtime: Long, visibility : String, event: MeetupEvent, group: MeetupGroup, member: MeetupMember, venue: MeetupVenue)\n",
    "\n",
    "\n",
    "class WebSocketReceiver(url: String, storageLevel: StorageLevel) extends Receiver[MeetupRsvp](storageLevel) with Logging {\n",
    "  private var webSocket: WebSocket = _\n",
    "  \n",
    "  def onStart() {\n",
    "    try{\n",
    "      logInfo(\"Connecting to WebSocket: \" + url)\n",
    "      val newWebSocket = WebSocket().open(url).onTextMessage({ msg: String => parseJson(msg) })\n",
    "      setWebSocket(newWebSocket)\n",
    "      logInfo(\"Connected to: WebSocket\" + url)\n",
    "    } catch {\n",
    "      case e: Exception => restart(\"Error starting WebSocket stream\", e)\n",
    "    }\n",
    "  }\n",
    "\n",
    "  def onStop() {\n",
    "    setWebSocket(null)\n",
    "    logInfo(\"WebSocket receiver stopped\")\n",
    "  }\n",
    "\n",
    "  private def setWebSocket(newWebSocket: WebSocket) = synchronized {\n",
    "    if (webSocket != null) {\n",
    "      webSocket.shutdown()\n",
    "    }\n",
    "    webSocket = newWebSocket\n",
    "  }\n",
    "\n",
    "  private def parseJson(jsonStr: String): Unit =\n",
    "  {\n",
    "    try {\n",
    "      implicit lazy val formats = DefaultFormats\n",
    "      val json = parse(jsonStr)\n",
    "      val rsvp = json.extract[MeetupRsvp]\n",
    "      store(rsvp)\n",
    "    } catch {\n",
    "       case e: Exception => e.getMessage()\n",
    "    }\n",
    "  }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Persist the Meetup stream to IBM Cloudant\n",
    "This function creates a streaming context that operates on a 10 second window.  Note due to our implementation in the custom WebSocketReceiver we are able to transform the text content of the websocket to JSON and extract the MeetupRsvp from it.  Then for each MeetupRsvp RDD in our stream we are able to filter the stream where the group_state equals Texas.  Lastly we convert the MeetupRsvp to a dataframe and utilize our writeToDatabase function to persist the instance to IBM Cloudant.  As this is a quick demo we set the timeout of the streaming context to be one minute so we can move onto our analysis of the data in IBM Cloudant."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Use dbName=meetup_group, indexName=null, jsonstore.rdd.partitions=5, jsonstore.rdd.maxInPartition=-1, jsonstore.rdd.minInPartition=10, jsonstore.rdd.requestTimeout=100000,jsonstore.rdd.concurrentSave=-1,jsonstore.rdd.bulkSize=1\n",
      "+--------+--------------------+----------+-------------+-----------+--------------------+---------+---------+--------------------+\n",
      "|group_id|          group_name|group_city|group_country|group_state|       group_urlname|group_lat|group_lon|        group_topics|\n",
      "+--------+--------------------+----------+-------------+-----------+--------------------+---------+---------+--------------------+\n",
      "| 3448252|Network After Wor...|    Austin|           us|         TX|NetworkAfterWork-...|    30.27|   -97.74|List([Professiona...|\n",
      "+--------+--------------------+----------+-------------+-----------+--------------------+---------+---------+--------------------+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "false"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Use dbName=meetup_group, indexName=null, jsonstore.rdd.partitions=5, jsonstore.rdd.maxInPartition=-1, jsonstore.rdd.minInPartition=10, jsonstore.rdd.requestTimeout=100000,jsonstore.rdd.concurrentSave=-1,jsonstore.rdd.bulkSize=1\n",
      "+--------+--------------------+----------+-------------+-----------+----------------+---------+---------+--------------------+\n",
      "|group_id|          group_name|group_city|group_country|group_state|   group_urlname|group_lat|group_lon|        group_topics|\n",
      "+--------+--------------------+----------+-------------+-----------+----------------+---------+---------+--------------------+\n",
      "| 2672242|OpenStack Austin ...|    Austin|           us|         TX|OpenStack-Austin|     30.4|   -97.75|List([Virtualizat...|\n",
      "+--------+--------------------+----------+-------------+-----------+----------------+---------+---------+--------------------+\n",
      "\n",
      "Use dbName=meetup_group, indexName=null, jsonstore.rdd.partitions=5, jsonstore.rdd.maxInPartition=-1, jsonstore.rdd.minInPartition=10, jsonstore.rdd.requestTimeout=100000,jsonstore.rdd.concurrentSave=-1,jsonstore.rdd.bulkSize=1\n",
      "+--------+--------------------+----------+-------------+-----------+--------------------+---------+---------+--------------------+\n",
      "|group_id|          group_name|group_city|group_country|group_state|       group_urlname|group_lat|group_lon|        group_topics|\n",
      "+--------+--------------------+----------+-------------+-----------+--------------------+---------+---------+--------------------+\n",
      "|11106032|Holistic & Energy...|Richardson|           us|         TX|Holistic-Energy-H...|    32.96|   -96.75|List([Alternative...|\n",
      "+--------+--------------------+----------+-------------+-----------+--------------------+---------+---------+--------------------+\n",
      "\n",
      "Use dbName=meetup_group, indexName=null, jsonstore.rdd.partitions=5, jsonstore.rdd.maxInPartition=-1, jsonstore.rdd.minInPartition=10, jsonstore.rdd.requestTimeout=100000,jsonstore.rdd.concurrentSave=-1,jsonstore.rdd.bulkSize=1\n",
      "+--------+--------------------+----------+-------------+-----------+-------------+---------+---------+--------------------+\n",
      "|group_id|          group_name|group_city|group_country|group_state|group_urlname|group_lat|group_lon|        group_topics|\n",
      "+--------+--------------------+----------+-------------+-----------+-------------+---------+---------+--------------------+\n",
      "| 3282852|Dallas Agile Lead...|    Irving|           us|         TX|   Dallas-ALN|    32.85|   -96.96|List([Self-Improv...|\n",
      "+--------+--------------------+----------+-------------+-----------+-------------+---------+---------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def persistStream(conf: SparkConf) = {\n",
    "    val ssc = new StreamingContext(conf, Seconds(10))\n",
    "    val stream = ssc.receiverStream[MeetupRsvp](new WebSocketReceiver(\"ws://stream.meetup.com/2/rsvps\", StorageLevel.MEMORY_ONLY_SER))\n",
    "    stream.foreachRDD((rdd: RDD[MeetupRsvp], time: Time) => {\n",
    "      val sqlContext = new SQLContext(rdd.sparkContext)  \n",
    "      import sqlContext.implicits._\n",
    "      val df = rdd.map(data => {\n",
    "        data.group\n",
    "      }).filter(_.group_state.getOrElse(\"\").equals(\"TX\")).toDF()\n",
    "      if(df.collect().length > 0) {\n",
    "        writeToDatabse(\"meetup_group\", df)\n",
    "        df.show()\n",
    "      }\n",
    "    })\n",
    "    ssc.start()\n",
    "    ssc.awaitTerminationOrTimeout(60000)\n",
    "}\n",
    "\n",
    "persistStream(conf)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Scala 2.10",
   "language": "scala",
   "name": "spark"
  },
  "language_info": {
   "name": "scala"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}