{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tools for HDF FIle Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gather Phase Tools\n",
    "\n",
    "This importable notebook provides the tooling necessary to handle the processing for the **Gather Phases** in the ETL process for the NOAA HDTA project. This tooling supports Approaches 1 and 2 using **HDF files**. \n",
    "\n",
    "Each of the process phases require a dictionary to drive the workflow. \n",
    "\n",
    "```\n",
    "project_layout = { \n",
    "    'Content_Version': '',\n",
    "    'Daily_Input_Files': '',\n",
    "    'Raw_Details': '',\n",
    "    'Missing_Details': '',\n",
    "    'Station_Summary': '', \n",
    "    'Station_Details': '',\n",
    "    }\n",
    "\n",
    "```\n",
    "Process Phase | Function to run \n",
    "--- | --- \n",
    "Phase 1 Approach 2 | noaa_run_phase1_approach2(project_layout)\n",
    "Phase 2 Approach 2 | noaa_run_phase2_approach2(project_layout)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# <help>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# <api>\n",
    "\n",
    "import os\n",
    "import time\n",
    "import glob\n",
    "import struct\n",
    "import collections\n",
    "import pandas as pd\n",
    "\n",
    "# Create a collection for returning multiple lists of tuples\n",
    "approach1_bag = collections.namedtuple('GatherBag', ['raw', 'missing'])\n",
    "\n",
    "# Historical Raw Daily Detail\n",
    "raw_daily_detail_rec_template = {'StationID': \"\",\n",
    "                                    'Year': \"\",\n",
    "                                    'Month': \"\",\n",
    "                                    'Day': \"\",\n",
    "                                    'Type': \"\",\n",
    "                                    'FahrenheitTemp': \"\",\n",
    "                                }\n",
    "\n",
    "# Historical Daily Missing Record Detail\n",
    "missing_detail_rec_template = {'StationID': \"\",\n",
    "                                'Year': \"\",\n",
    "                                'Month': \"\",\n",
    "                                'Day': \"\",\n",
    "                                'Type': \"\",\n",
    "                               }\n",
    "\n",
    "def get_filename(pathname):\n",
    "    '''Fetch filename portion of pathname.'''\n",
    "    plist = pathname.split('/')\n",
    "    fname, fext = os.path.splitext(plist[len(plist)-1])\n",
    "    return fname\n",
    "\n",
    "def elapsed_time(secs):\n",
    "    '''Compute formated time stamp given seconds elapsed. '''\n",
    "    m, s = divmod(secs, 60)\n",
    "    h, m = divmod(m, 60)\n",
    "    et = \"%d:%02d:%02d\" % (h, m, s)\n",
    "    return et\n",
    "\n",
    "def noaa_convert_c2f(noaa_temp):\n",
    "    '''Returns Fahrenheit temperature value from a NOAA temperature (tenths of degrees C) '''\n",
    "    celsius = int(noaa_temp)/10\n",
    "    fahrenheit = 9.0/5.0 * celsius + 32\n",
    "    return round(fahrenheit,1)\n",
    "\n",
    "def noaa_gather_lines(lines):\n",
    "    '''Return dataframes for raw and missing detail from list of lines.'''\n",
    "    # Create list of tuples \n",
    "    raw_list = []\n",
    "    missing_list = []\n",
    "    for index, line in enumerate(lines):\n",
    "        #print(\"Processing line {0}.\").format(index)\n",
    "        r = noaa_gather_daily_detail(line)\n",
    "        raw_list += r.raw\n",
    "        missing_list += r.missing\n",
    "    # Construct dataframes\n",
    "    df_raw = pd.DataFrame(raw_list)\n",
    "    df_missing = pd.DataFrame(missing_list)\n",
    "    return approach1_bag(df_raw, df_missing)\n",
    "\n",
    "def noaa_gather_daily_detail(line):\n",
    "    '''Extract content from daily record, create raw and missing tuples.'''\n",
    "    station_time_element = struct.unpack('11s4s2s4s', line[0:21])\n",
    "    raw_tuple_list = []\n",
    "    missing_tuple_list = []\n",
    "    if station_time_element[3] == 'TMIN' or station_time_element[3] == 'TMAX':\n",
    "        values = line[21:]\n",
    "        day_of_month = 0\n",
    "        while(len(values) > 7):\n",
    "            day_of_month = day_of_month + 1\n",
    "            day_measure = struct.unpack('5ssss', values[0:8])\n",
    "            if day_measure[0] != '-9999':\n",
    "                raw_tuple = dict(raw_daily_detail_rec_template)\n",
    "                # Compute degrees fahrenheit\n",
    "                fahrenheit = noaa_convert_c2f(int(day_measure[0]))\n",
    "                # Construct raw detail record\n",
    "                raw_tuple['StationID'] = station_time_element[0]\n",
    "                raw_tuple['Year'] = station_time_element[1]\n",
    "                raw_tuple['Month']= station_time_element[2]\n",
    "                raw_tuple['Day'] = day_of_month\n",
    "                raw_tuple['Type'] = station_time_element[3]\n",
    "                raw_tuple['FahrenheitTemp'] = fahrenheit\n",
    "                raw_tuple_list.append(raw_tuple)\n",
    "            else:\n",
    "                # Construct missing detail record\n",
    "                missing_tuple = dict(missing_detail_rec_template)\n",
    "                missing_tuple['StationID'] = station_time_element[0]\n",
    "                missing_tuple['Year'] = station_time_element[1]\n",
    "                missing_tuple['Month']= station_time_element[2]\n",
    "                missing_tuple['Day'] = day_of_month\n",
    "                missing_tuple['Type'] = station_time_element[3]\n",
    "                missing_tuple_list.append(missing_tuple)\n",
    "            # Adjust offest for next day\n",
    "            values = values[8:]    \n",
    "    # Return new tuples\n",
    "    return approach1_bag(raw_tuple_list, missing_tuple_list)    \n",
    "\n",
    "def noaa_process_hcn_daily_file(fname):\n",
    "    '''Return dataframes for raw and missing detail from lines in file.'''\n",
    "    print(\"Extracting content from file {0}.\").format(fname)\n",
    "    x = 0\n",
    "    raw_cols = ['StationID', 'Year', 'Month', 'Day', 'Type', 'FahrenheitTemp']\n",
    "    missing_cols = ['StationID', 'Year', 'Month', 'Day', 'Type']\n",
    "    # Create list of tuples \n",
    "    raw_list = []\n",
    "    missing_list = []\n",
    "    # Start Timer\n",
    "    start_time = time.time()\n",
    "    with open(fname,'r') as f:\n",
    "        lines = f.readlines()\n",
    "        # Changed next 2 lines only.\n",
    "        for line in lines:\n",
    "            x += 1\n",
    "            r = noaa_gather_daily_detail(line)\n",
    "            raw_list += r.raw\n",
    "            missing_list += r.missing\n",
    "    f.close() \n",
    "    seconds = (time.time() - start_time)\n",
    "    print(\">> Processing Complete: {0} lines of file {1}.\").format(x, fname)\n",
    "    print(\">>   Elapsed file execution time {0}\").format(elapsed_time(seconds))\n",
    "    # Capture and Sort Results in DataFrames\n",
    "    df_raw = pd.DataFrame(raw_list)\n",
    "    df_missing = pd.DataFrame(missing_list)\n",
    "    r = df_raw.sort(raw_cols).reindex(columns=raw_cols)\n",
    "    m = df_missing.sort(missing_cols).reindex(columns=missing_cols)\n",
    "    return approach1_bag(r, m)\n",
    "\n",
    "def noaa_run_phase1_approach2(project_layout):\n",
    "    '''Process corpus of daily files and store results in HDF file.'''\n",
    "    raw_store = None\n",
    "    missing_store = None\n",
    "    try:\n",
    "        if os.path.isfile(project_layout['Raw_Details']):\n",
    "            raise Exception(\"Raw Details file already exists.\")\n",
    "        if os.path.isfile(project_layout['Missing_Details']):\n",
    "            raise Exception(\"Missing Details file already exists.\")\n",
    "        # Start Timer\n",
    "        start_time = time.time()\n",
    "        raw_store = pd.HDFStore(project_layout['Raw_Details'],'w')\n",
    "        missing_store = pd.HDFStore(project_layout['Missing_Details'],'w')\n",
    "        for index, fname in enumerate(glob.glob(project_layout['Daily_Input_Files'])):\n",
    "            f = get_filename(fname)\n",
    "            print(\">> Processing file {0}: {1}\").format(index, f)\n",
    "            r = noaa_process_hcn_daily_file(fname)\n",
    "            raw_store.put('noaa_hdta_raw_detail/' + \n",
    "                          project_layout['Content_Version']  + \n",
    "                          '/' + f, r.raw\n",
    "                         )\n",
    "            missing_store.put('noaa_hdta_missing_detail/' +\n",
    "                              project_layout['Content_Version'] + '/'\n",
    "                              + f, r.missing\n",
    "                             )\n",
    "        raw_store.close()\n",
    "        missing_store.close()\n",
    "        seconds = (time.time() - start_time)\n",
    "        print(\">>      Processing Complete.\")\n",
    "        print(\">>   Elapsed corpus execution time {0}\").format(elapsed_time(seconds))\n",
    "    except Exception as e:\n",
    "        if raw_store:\n",
    "            raw_store.close()\n",
    "        if missing_store:\n",
    "            missing_store.close()\n",
    "        print(\">> Processing Failed: Error {0}\").format(e.message)\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### noaa_run_phase1_approach2\n",
    "Takes a dictionary of project folder details to drive the processing of *Gather Phase 1 Approach 2* using **HDF files**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# <help:noaa_run_phase1_approach2>\n",
    "project_layout = { \n",
    "    'Content_Version': '',\n",
    "    'Daily_Input_Files': '',\n",
    "    'Raw_Details': '',\n",
    "    'Missing_Details': '',\n",
    "    'Station_Summary': '', \n",
    "    'Station_Details': '',\n",
    "    }\n",
    "noaa_run_phase1_approach2(project_layout)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# <api>\n",
    "\n",
    "import os\n",
    "import time\n",
    "import datetime\n",
    "import collections\n",
    "import pandas as pd\n",
    "import traceback\n",
    "\n",
    "# Create a collection for returning multiple lists of tuples\n",
    "approach2_bag = collections.namedtuple('GatherBag', ['DailySummary', 'DailyDetail'])\n",
    "\n",
    "# Historical Daily Summary\n",
    "summary_template = {'StationID': \"\", \n",
    "                    'Month': \"\", \n",
    "                    'Day': \"\",\n",
    "                    'FirstYearOfRecord': \"\", \n",
    "                    'TMin': \"\", \n",
    "                    'TMinRecordYear': \"\", \n",
    "                    'TMax': \"\", \n",
    "                    'TMaxRecordYear': \"\", \n",
    "                    'CurTMinMaxDelta': \"\", \n",
    "                    'CurTMinRecordDur': \"\", \n",
    "                    'CurTMaxRecordDur': \"\", \n",
    "                    'MaxDurTMinRecord': \"\", \n",
    "                    'MinDurTMinRecord': \"\", \n",
    "                    'MaxDurTMaxRecord': \"\", \n",
    "                    'MinDurTMaxRecord': \"\", \n",
    "                    'TMinRecordCount': \"\", \n",
    "                    'TMaxRecordCount': \"\" \n",
    "                    }\n",
    "\n",
    "summary_cols = ['StationID', 'Month', 'Day', 'FirstYearOfRecord', \n",
    "                'TMin', 'TMinRecordYear', 'TMax', 'TMaxRecordYear',\n",
    "                'CurTMinMaxDelta', 'CurTMinRecordDur','CurTMaxRecordDur',\n",
    "                'MaxDurTMinRecord', 'MinDurTMinRecord',\n",
    "                'MaxDurTMaxRecord', 'MinDurTMaxRecord',\n",
    "                'TMinRecordCount', 'TMaxRecordCount'            \n",
    "                ]\n",
    "\n",
    "# Historical Daily Detail\n",
    "detail_template = {'StationID': \"\",\n",
    "                    'Year': \"\",\n",
    "                    'Month': \"\",\n",
    "                    'Day': \"\",\n",
    "                    'Type': \"\",\n",
    "                    'OldTemp': \"\",\n",
    "                    'NewTemp': \"\",\n",
    "                    'TDelta': \"\"\n",
    "                    }\n",
    "\n",
    "detail_cols = ['StationID', 'Year', 'Month', 'Day', 'Type', \n",
    "               'NewTemp', 'OldTemp', 'TDelta'\n",
    "              ]\n",
    "\n",
    "def get_filename(pathname):\n",
    "    '''Fetch filename portion of pathname.'''\n",
    "    plist = pathname.split('/')\n",
    "    fname, fext = os.path.splitext(plist[len(plist)-1])\n",
    "    return fname\n",
    "\n",
    "def elapsed_time(secs):\n",
    "    '''Compute formated time stamp given seconds elapsed. '''\n",
    "    m, s = divmod(secs, 60)\n",
    "    h, m = divmod(m, 60)\n",
    "    et = \"%d:%02d:%02d\" % (h, m, s)\n",
    "    return et\n",
    "\n",
    "def get_key_list(hdf5file, type='raw_detail'):\n",
    "    '''Return a list of keys for requested type from specified HDF file.'''\n",
    "    print(\"Fetching keys for type = {0}\").format(type)\n",
    "    keylist = []\n",
    "    store = None\n",
    "    try:\n",
    "        store = pd.HDFStore(hdf5file,'r')\n",
    "        h5keys = store.keys()\n",
    "        store.close()\n",
    "        for k in h5keys:\n",
    "            if k.find(type) > -1:\n",
    "                keylist.append(k)\n",
    "    except:\n",
    "        if store:\n",
    "            store.close()\n",
    "        raise\n",
    "    return keylist\n",
    "\n",
    "def cleans_invalid_days(df):\n",
    "    '''Return a dataframe void of invalid days'''\n",
    "    ShortMths = {4,6,9,11}\n",
    "    df_clean = df.query('(((Month not in @ShortMths) & (Day != 31)) and ((Month != 2) or (Day < 30)) )')\n",
    "    return df_clean\n",
    "\n",
    "def noaa_gather2_process_records(raw_tuples):\n",
    "    '''Compute formated time stamp given seconds elapsed. '''\n",
    "    # Sample Tuple:\n",
    "    # (0, 'USC00011084', '1926', '01', 21, 'TMAX', 73.400000000000006)\n",
    "    #\n",
    "    # Create several 12x31 matrices to store daily detail per metric of interest.\n",
    "    fyr_online_for_day = [[9999 for x in range(32)] for x in range(13)]  \n",
    "    tmin_history = [[-99 for x in range(32)] for x in range(13)] \n",
    "    tmax_history = [[-99 for x in range(32)] for x in range(13)] \n",
    "    tminyr_history = [[-99 for x in range(32)] for x in range(13)] \n",
    "    tmaxyr_history = [[-99 for x in range(32)] for x in range(13)] \n",
    "    tminrc_history = [[0 for x in range(32)] for x in range(13)]  \n",
    "    tmaxrc_history = [[0 for x in range(32)] for x in range(13)]  \n",
    "    tmax_max_life = [[0 for x in range(32)] for x in range(13)] \n",
    "    tmax_min_life = [[9999 for x in range(32)] for x in range(13)] \n",
    "    tmin_max_life = [[0 for x in range(32)] for x in range(13)]  \n",
    "    tmin_min_life = [[9999 for x in range(32)] for x in range(13)]   \n",
    "    # Capture Station ID (all raw-tuples are per station)\n",
    "    station_ID  = raw_tuples[0][1]\n",
    "    # Process each raw daily tuple: create daily retail tuples while updating matrices.\n",
    "    detail_list = []\n",
    "    for t in raw_tuples:\n",
    "        detail_row = dict(detail_template)\n",
    "        detail_row['StationID'] = t[1]\n",
    "        detail_row['Year'] = t[2]\n",
    "        detail_row['Month'] = t[3]\n",
    "        detail_row['Day'] = str(t[4])\n",
    "        month = int(t[3])-1\n",
    "        day = t[4]-1\n",
    "        # For this day, what was the first year in which this station was operational?\n",
    "        if fyr_online_for_day[month][day] > int(t[2]):\n",
    "            fyr_online_for_day[month][day] = int(t[2])\n",
    "        # Handle TMAX\n",
    "        if (t[5] == 'TMAX'):\n",
    "            # Handle TMAX for first record\n",
    "            if (tmax_history[month][day] == -99):\n",
    "                # Handle TMAX for first \n",
    "                detail_row['Type'] = 'TMAX'\n",
    "                detail_row['OldTemp'] = round(t[6],1)\n",
    "                detail_row['NewTemp'] = round(t[6],1)\n",
    "                detail_row['TDelta'] = 0\n",
    "                tmax_history[month][day] = round(t[6],1)\n",
    "                tmaxyr_history[month][day] = int(t[2])\n",
    "                tmaxrc_history[month][day] = 1\n",
    "                tmax_min_life[month][day] = 0\n",
    "                tmax_max_life[month][day] = 0\n",
    "                # Add new daily detail row\n",
    "                detail_list.append(detail_row)\n",
    "            # Handle TMAX for new daily record\n",
    "            elif (round(t[6],1) > tmax_history[month][day]):\n",
    "                detail_row['Type'] = 'TMAX'\n",
    "                detail_row['OldTemp'] = tmax_history[month][day]\n",
    "                detail_row['NewTemp'] = round(t[6],1)\n",
    "                detail_row['TDelta'] = round(t[6],1) - tmax_history[month][day]\n",
    "                current_tmin_duration = int(t[2]) - tminyr_history[month][day]\n",
    "                current_tmax_duration = int(t[2]) - tmaxyr_history[month][day]\n",
    "                if tmin_max_life[month][day] == 0:\n",
    "                    tmin_max_life[month][day] = int(t[2]) - fyr_online_for_day[month][day]\n",
    "                if tmax_max_life[month][day] == 0:\n",
    "                    tmax_max_life[month][day] = int(t[2]) - fyr_online_for_day[month][day]\n",
    "                if current_tmax_duration > tmax_max_life[month][day]:\n",
    "                    tmax_max_life[month][day] = current_tmax_duration\n",
    "                if current_tmin_duration < tmin_max_life[month][day]:\n",
    "                    tmin_max_life[month][day] = current_tmax_duration\n",
    "                tmax_history[month][day] = round(t[6],1)\n",
    "                tmaxyr_history[month][day] = int(t[2])\n",
    "                tmaxrc_history[month][day] += 1\n",
    "                # Add new daily detail row\n",
    "                detail_list.append(detail_row)\n",
    "        if (t[5] == 'TMIN'):\n",
    "            # Handle TMIN for first record\n",
    "            if (tmin_history[month][day] == -99):\n",
    "                # Handle TMIN for first \n",
    "                detail_row['Type'] = 'TMIN'\n",
    "                detail_row['OldTemp'] = round(t[6],1)\n",
    "                detail_row['NewTemp'] = round(t[6],1)\n",
    "                detail_row['TDelta'] = 0\n",
    "                tmin_history[month][day] = round(t[6],1)\n",
    "                tminyr_history[month][day] = int(t[2])\n",
    "                tminrc_history[month][day] = 1\n",
    "                tmin_min_life[month][day] = 0\n",
    "                tmin_max_life[month][day] = 0\n",
    "                # Add new daily detail row\n",
    "                detail_list.append(detail_row)\n",
    "            # Handle TMIN for new daily record\n",
    "            elif (round(t[6],1) < tmin_history[month][day]):\n",
    "                detail_row['Type'] = 'TMIN'\n",
    "                detail_row['OldTemp'] = tmin_history[month][day]\n",
    "                detail_row['NewTemp'] = round(t[6],1)\n",
    "                detail_row['TDelta'] = tmin_history[month][day] - round(t[6],1)\n",
    "                current_tmin_duration = int(t[2]) - tminyr_history[month][day]\n",
    "                current_tmax_duration = int(t[2]) - tmaxyr_history[month][day]\n",
    "                if tmax_min_life[month][day] == 0:\n",
    "                    tmax_min_life[month][day] = int(t[2]) - fyr_online_for_day[month][day]\n",
    "                if tmin_min_life[month][day] == 0:\n",
    "                    tmin_min_life[month][day] = int(t[2]) - fyr_online_for_day[month][day]\n",
    "                if current_tmax_duration > tmax_min_life[month][day]:\n",
    "                    tmax_min_life[month][day] = current_tmin_duration\n",
    "                if current_tmin_duration < tmin_min_life[month][day]:\n",
    "                    tmin_min_life[month][day] = current_tmin_duration\n",
    "                tmin_history[month][day] = round(t[6],1)\n",
    "                tminyr_history[month][day] = int(t[2])\n",
    "                tminrc_history[month][day] += 1\n",
    "                # Add new daily detail row\n",
    "                detail_list.append(detail_row)\n",
    "    # Create a daily summary record for each day of the year using our matrices.\n",
    "    summary_list = []\n",
    "    now = datetime.datetime.now()\n",
    "    for mth in xrange(1,13):\n",
    "        for day in xrange(1,32):\n",
    "            m = mth-1\n",
    "            d= day-1\n",
    "            summary_row = dict(summary_template)\n",
    "            summary_row['StationID'] = station_ID\n",
    "            summary_row['Month'] = mth\n",
    "            summary_row['Day'] = day\n",
    "            summary_row['FirstYearOfRecord'] = fyr_online_for_day[m][d]\n",
    "            summary_row['TMin'] = tmin_history[m][d]\n",
    "            summary_row['TMinRecordYear'] = tminyr_history[m][d]\n",
    "            summary_row['TMax'] = tmax_history[m][d] \n",
    "            summary_row['TMaxRecordYear'] = tmaxyr_history[m][d]\n",
    "            summary_row['CurTMinMaxDelta'] = summary_row['TMax'] - summary_row['TMin']\n",
    "            summary_row['CurTMinRecordDur'] = int(now.year) - summary_row['TMinRecordYear']\n",
    "            summary_row['CurTMaxRecordDur'] = int(now.year) - summary_row['TMaxRecordYear']\n",
    "            summary_row['MaxDurTMinRecord'] = tmax_min_life[m][d] # Can not explain\n",
    "            summary_row['MinDurTMinRecord'] = tmin_min_life[m][d]\n",
    "            summary_row['MaxDurTMaxRecord'] = tmax_max_life[m][d]\n",
    "            summary_row['MinDurTMaxRecord'] = tmin_max_life[m][d] # Can not explain       \n",
    "            summary_row['TMinRecordCount'] = tminrc_history[m][d]\n",
    "            summary_row['TMaxRecordCount'] = tmaxrc_history[m][d]\n",
    "            # Add new daily summary row\n",
    "            summary_list.append(summary_row)\n",
    "    return approach2_bag(summary_list, detail_list)\n",
    "\n",
    "def noaa_run_phase2_approach2(project_layout,create_details=False):\n",
    "    '''Parse H5 dataset to create derived datasets.'''\n",
    "    raw_store = None\n",
    "    summary_store = None\n",
    "    detail_store = None\n",
    "    try:\n",
    "        if not os.path.isfile(project_layout['Raw_Details']):\n",
    "            raise Exception(\"Raw Details file does not exist.\")\n",
    "        if os.path.isfile(project_layout['Station_Summary']):\n",
    "            raise Exception(\"Station Summary file already exists.\")\n",
    "        if create_details and os.path.isfile(project_layout['Station_Details']):\n",
    "            raise Exception(\"Station Details file already exists.\")\n",
    "        # Start Key Fetch Timer\n",
    "        start_time = time.time()\n",
    "        keys = get_key_list(project_layout['Raw_Details'])\n",
    "        seconds = (time.time() - start_time)\n",
    "        print(\">> Fetch Complete.\")\n",
    "        print(\">>   Elapsed key-fetch execution time {0}\").format(elapsed_time(seconds))   \n",
    "        # Start Key Processing Timer\n",
    "        start_time = time.time()\n",
    "        summary_store = pd.HDFStore(project_layout['Station_Summary'],'w')\n",
    "        if create_details:\n",
    "            detail_store = pd.HDFStore(project_layout['Station_Details'],'w')\n",
    "        raw_store = pd.HDFStore(project_layout['Raw_Details'],'r')\n",
    "        for index, k in enumerate(keys):\n",
    "            f = get_filename(k)\n",
    "            print(\"Processing dataset {0} - {1}: {2}\").format(index,len(keys),f)\n",
    "            ds = raw_store.get(k)\n",
    "            raw_tuples = list(ds.itertuples())\n",
    "            r = noaa_gather2_process_records(raw_tuples)\n",
    "            # Capture results and store dataframes in H5 files\n",
    "            df_summary = pd.DataFrame(r.DailySummary).sort(summary_cols).reindex(columns=summary_cols)\n",
    "            df_cleaned_summary = cleans_invalid_days(df_summary)\n",
    "            summary_store.put('noaa_hdta_station_summary/' + \n",
    "                              project_layout['Content_Version']  + \n",
    "                              '/' + f, df_cleaned_summary\n",
    "                             )\n",
    "            if create_details:\n",
    "                df_detail = pd.DataFrame(r.DailyDetail).sort(detail_cols).reindex(columns=detail_cols)\n",
    "                df_cleaned_detail = cleans_invalid_days(df_detail)\n",
    "                detail_store.put('noaa_hdta_station_daily_detail/' +\n",
    "                                  project_layout['Content_Version'] + '/'\n",
    "                                  + f, df_cleaned_detail\n",
    "                                 )\n",
    "        raw_store.close()\n",
    "        summary_store.close()\n",
    "        if create_details:\n",
    "            detail_store.close()\n",
    "        seconds = (time.time() - start_time)\n",
    "        print(\">> Processing Complete.\")\n",
    "        print(\">>   Elapsed corpus execution time {0}\").format(elapsed_time(seconds))   \n",
    "    except Exception as e:\n",
    "        if raw_store:\n",
    "            raw_store.close()\n",
    "        if summary_store:\n",
    "            summary_store.close()\n",
    "        if detail_store:\n",
    "            detail_store.close()\n",
    "        var = traceback.format_exc()\n",
    "        print var\n",
    "        print(\">> Processing Failed: Error {0}\").format(e.message)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### noaa_run_phase2_approach2\n",
    "Takes a dictionary of project folder details to drive the processing of *Gather Phase 2 Approach 2* using **HDF files**.\n",
    "\n",
    "#### Disk Storage Requirements\n",
    "\n",
    "* This function creates a **Station Summaries** dataset that requires ~2GB of free space. \n",
    "* This function can also create a **Station Details** dataset. If you require this dataset to be generated, modify the call to ```noaa_run_phase2_approach2()``` with ```create_details=True```. You will need additional free space to support this feature. Estimated requirement: <font color=\"red\">**5GB**</font>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# <help:noaa_run_phase2_approach2>\n",
    "project_layout = { \n",
    "    'Content_Version': '',\n",
    "    'Daily_Input_Files': '',\n",
    "    'Raw_Details': '',\n",
    "    'Missing_Details': '',\n",
    "    'Station_Summary': '', \n",
    "    'Station_Details': '',\n",
    "    }\n",
    "noaa_run_phase2_approach2(project_layout)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
